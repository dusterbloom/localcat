"""
HotMem V3 Training Colab for Qwen2.5-0.5B
Optimized for high-quality relation extraction in localcat pipeline

🎯 TRAINING GOAL:
- Train Qwen2.5-0.5B to extract entities and relations from conversational text
- Output clean JSON format compatible with HotMem V3 streaming extraction
- Achieve high accuracy on real-world conversational data
- Optimize for speed and efficiency on local hardware

📋 PIPELINE COMPATIBILITY:
- Input: Conversational text (voice transcripts, chat messages)
- Output: JSON with entities and relations arrays
- Format: {"entities": [{"text": "Entity", "type": "PERSON", "confidence": 0.9}], "relations": [{"subject": "Entity1", "predicate": "works_for", "object": "Entity2", "confidence": 0.8}]}
- Integration: HotMem V3 streaming extraction and dual graph architecture

🚀 USAGE:
1. Upload this to Google Colab
2. Set runtime to GPU (T4 recommended)
3. Run all cells
4. Download trained model to /optimized_models/hotmem_v3_package/
"""

# Step 1: Install dependencies
print("🔧 Installing dependencies...")
!pip install -q unsloth datasets torch accelerate
!pip install -q git+https://github.com/huggingface/transformers.git
!pip install -q git+https://github.com/huggingface/peft.git
!pip install -q bitsandbytes trl
!pip install -q wandb tensorboard

print("✅ Dependencies installed")

# Step 2: Import libraries
import torch
from datasets import load_dataset, concatenate_datasets, Dataset
from unsloth import FastLanguageModel
import json
import os
import re
from tqdm import tqdm
import wandb
from typing import Dict, List, Any, Optional
import random
import numpy as np

print("✅ Libraries imported")

# Step 3: Configuration
class TrainingConfig:
    """Training configuration optimized for Qwen2.5-0.5B"""
    
    # Model configuration
    model_name = "Qwen/Qwen2.5-0.5B-Instruct"
    max_seq_length = 2048
    dtype = torch.float16  # or None for auto detection
    load_in_4bit = True
    
    # Training parameters
    learning_rate = 2e-4
    batch_size = 4
    gradient_accumulation_steps = 4
    warmup_steps = 100
    max_steps = 2000
    weight_decay = 0.01
    lr_scheduler_type = "cosine"
    optim = "adamw_8bit"
    
    # LoRA configuration
    r = 16
    lora_alpha = 16
    lora_dropout = 0.05
    bias = "none"
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
    # Data configuration
    train_test_split = 0.95
    max_examples_per_dataset = 50000
    
    # Output configuration
    output_dir = "./hotmem_v3_qwen"
    final_model_dir = "./hotmem_v3_final"
    upload_to_hub = False
    
    # WandB configuration
    use_wandb = True
    wandb_project = "hotmem-v3-training"
    wandb_run_name = "qwen2.5-0.5b-relation-extraction"

print("✅ Configuration loaded")

# Step 4: Data loading and preprocessing
def load_and_prepare_datasets():
    """Load and prepare diverse datasets for high-quality training"""
    
    datasets_dict = {}
    
    # Primary dataset: REBEL (high-quality relation extraction)
    print("Loading REBEL dataset...")
    try:
        rebel = load_dataset("Babelscape/rebel-dataset", split="train[:40000]")
        datasets_dict['rebel'] = rebel
        print(f"✅ REBEL: {len(rebel)} examples")
    except Exception as e:
        print(f"❌ REBEL loading failed: {e}")
    
    # Conversational relation extraction
    print("Loading conversational datasets...")
    try:
        # DialogRE for dialogue relations
        dialogre = load_dataset("dialogre", split="train[:20000]")
        datasets_dict['dialogre'] = dialogre
        print(f"✅ DialogRE: {len(dialogre)} examples")
    except Exception as e:
        print(f"❌ DialogRE loading failed: {e}")
    
    # Additional relation extraction datasets
    try:
        # Relation extraction QA
        rel_qa = load_dataset("lucadiliello/relationextractionqa", split="train[:10000]")
        datasets_dict['rel_qa'] = rel_qa
        print(f"✅ Relation QA: {len(rel_qa)} examples")
    except Exception as e:
        print(f"❌ Relation QA loading failed: {e}")
    
    # Generate synthetic conversational data
    print("Generating synthetic conversational data...")
    synthetic_data = generate_synthetic_conversational_data(20000)
    datasets_dict['synthetic'] = Dataset.from_list(synthetic_data)
    print(f"✅ Synthetic: {len(synthetic_data)} examples")
    
    return datasets_dict

def generate_synthetic_conversational_data(num_examples: int) -> List[Dict]:
    """Generate high-quality synthetic conversational data for training"""
    
    templates = [
        # Person-Organization relations
        "I work at {company} as a {position}.",
        "My name is {person} and I'm employed by {company}.",
        "{person} works for {company} in the {department} department.",
        "I'm {person}, a {position} at {company}.",
        
        # Person-Person relations
        "{person1} is friends with {person2}.",
        "I met {person2} through {person1}.",
        "{person1} and {person2} are colleagues.",
        "{person1} manages {person2} at work.",
        
        # Location relations
        "{company} is located in {location}.",
        "I live in {city} and work in {location}.",
        "{company} has offices in {location}.",
        "The {company} headquarters is in {location}.",
        
        # Product relations
        "{company} develops {product}.",
        "I use {product} from {company}.",
        "{company} launched {product} last year.",
        "{product} is made by {company}.",
    ]
    
    entities = {
        'person': ['John', 'Sarah', 'Mike', 'Emma', 'David', 'Lisa', 'Tom', 'Amy'],
        'company': ['Google', 'Microsoft', 'Apple', 'Amazon', 'Meta', 'Tesla', 'Netflix', 'Spotify'],
        'position': ['engineer', 'manager', 'developer', 'designer', 'analyst', 'director', 'specialist'],
        'department': ['engineering', 'marketing', 'sales', 'support', 'research', 'operations'],
        'location': ['San Francisco', 'New York', 'London', 'Tokyo', 'Paris', 'Berlin', 'Singapore'],
        'city': ['Seattle', 'Boston', 'Austin', 'Denver', 'Portland', 'Chicago', 'Los Angeles'],
        'product': ['iPhone', 'Windows', 'Android', 'Chrome', 'Safari', 'Office', 'Photoshop', 'Slack']
    }
    
    synthetic_data = []
    
    for _ in range(num_examples):
        template = random.choice(templates)
        
        # Fill in template with random entities
        text = template
        used_entities = {}
        
        for entity_type, entity_list in entities.items():
            if f'{{{entity_type}}' in text:
                entity = random.choice(entity_list)
                text = text.replace(f'{{{entity_type}}}', entity)
                if entity_type not in used_entities:
                    used_entities[entity_type] = []
                used_entities[entity_type].append(entity)
        
        # Extract entities and relations based on template
        entities_list = []
        relations_list = []
        
        entity_id = 0
        entity_map = {}
        
        # Add all mentioned entities
        for entity_type, entity_values in used_entities.items():
            for entity in entity_values:
                entity_map[entity] = {
                    'id': entity_id,
                    'text': entity,
                    'type': entity_type.upper(),
                    'confidence': 0.9
                }
                entities_list.append(entity_map[entity])
                entity_id += 1
        
        # Generate relations based on template patterns
        if 'work at' in text or 'employed by' in text:
            if 'person' in used_entities and 'company' in used_entities:
                person = used_entities['person'][0]
                company = used_entities['company'][0]
                relations_list.append({
                    'subject': person,
                    'predicate': 'works_for',
                    'object': company,
                    'confidence': 0.8
                })
        
        elif 'friends with' in text:
            if len(used_entities.get('person', [])) >= 2:
                person1 = used_entities['person'][0]
                person2 = used_entities['person'][1]
                relations_list.append({
                    'subject': person1,
                    'predicate': 'friends_with',
                    'object': person2,
                    'confidence': 0.8
                })
        
        elif 'located in' in text:
            if 'company' in used_entities and 'location' in used_entities:
                company = used_entities['company'][0]
                location = used_entities['location'][0]
                relations_list.append({
                    'subject': company,
                    'predicate': 'located_in',
                    'object': location,
                    'confidence': 0.8
                })
        
        elif 'develops' in text:
            if 'company' in used_entities and 'product' in used_entities:
                company = used_entities['company'][0]
                product = used_entities['product'][0]
                relations_list.append({
                    'subject': company,
                    'predicate': 'develops',
                    'object': product,
                    'confidence': 0.8
                })
        
        synthetic_data.append({
            'text': text,
            'entities': entities_list,
            'relations': relations_list,
            'confidence': 0.85
        })
    
    return synthetic_data

def format_for_training(example: Dict) -> Dict:
    """Format example for training with Qwen2.5-0.5B"""
    
    # Extract text and entities/relations
    text = example.get('text', '')
    entities = example.get('entities', [])
    relations = example.get('relations', [])
    
    # Create target JSON structure
    target_json = {
        'entities': entities,
        'relations': relations,
        'confidence': example.get('confidence', 0.8)
    }
    
    # Create training prompt
    system_prompt = """You are an expert relation extraction system. Extract entities and relations from the given text and output them in JSON format.

Entities should include people, organizations, locations, and other important nouns.
Relations should connect entities with meaningful predicates like 'works_for', 'located_in', 'develops', etc.

Output format:
{
  "entities": [
    {"text": "Entity Name", "type": "PERSON/ORG/LOC/PRODUCT", "confidence": 0.9}
  ],
  "relations": [
    {"subject": "Entity1", "predicate": "relation_type", "object": "Entity2", "confidence": 0.8}
  ],
  "confidence": 0.85
}"""

    user_prompt = f"Extract entities and relations from this text:\n\n{text}\n\nOutput JSON:"
    
    # Format for Qwen chat template
    formatted_text = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{user_prompt}<|im_end|>\n<|im_start|>assistant\n{json.dumps(target_json, indent=2)}<|im_end|>"
    
    return {
        'text': formatted_text,
        'target': json.dumps(target_json),
        'original_text': text,
        'entities': entities,
        'relations': relations
    }

def process_rebel_dataset(rebel_data):
    """Process REBEL dataset format"""
    processed_examples = []
    
    for example in rebel_data:
        try:
            # Extract text from REBEL format
            text = example.get('text', '')
            
            # Extract triplets
            triplets = example.get('triplets', [])
            
            entities = []
            relations = []
            entity_map = {}
            
            entity_id = 0
            for triplet in triplets:
                subject = triplet.get('subject', {}).get('label', '')
                object = triplet.get('object', {}).get('label', '')
                predicate = triplet.get('predicate', '')
                
                # Add entities if not already present
                if subject and subject not in entity_map:
                    entity_map[subject] = {
                        'text': subject,
                        'type': 'ENTITY',
                        'confidence': 0.9
                    }
                    entities.append(entity_map[subject])
                
                if object and object not in entity_map:
                    entity_map[object] = {
                        'text': object,
                        'type': 'ENTITY',
                        'confidence': 0.9
                    }
                    entities.append(entity_map[object])
                
                # Add relation
                if subject and object and predicate:
                    relations.append({
                        'subject': subject,
                        'predicate': predicate,
                        'object': object,
                        'confidence': 0.8
                    })
            
            processed_examples.append({
                'text': text,
                'entities': entities,
                'relations': relations,
                'confidence': 0.8
            })
            
        except Exception as e:
            print(f"Error processing REBEL example: {e}")
            continue
    
    return processed_examples

# Step 5: Load and prepare model
print("🚀 Loading model...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=TrainingConfig.model_name,
    max_seq_length=TrainingConfig.max_seq_length,
    dtype=TrainingConfig.dtype,
    load_in_4bit=TrainingConfig.load_in_4bit,
)

# Configure LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=TrainingConfig.r,
    target_modules=TrainingConfig.target_modules,
    lora_alpha=TrainingConfig.lora_alpha,
    lora_dropout=TrainingConfig.lora_dropout,
    bias=TrainingConfig.bias,
    use_gradient_checkpointing=True,
    random_state=42,
)

print("✅ Model loaded and LoRA configured")

# Step 6: Prepare training data
print("📚 Preparing training data...")
datasets_dict = load_and_prepare_datasets()

# Process all datasets
processed_datasets = []

for dataset_name, dataset in datasets_dict.items():
    print(f"Processing {dataset_name}...")
    
    if dataset_name == 'rebel':
        processed_data = process_rebel_dataset(dataset)
    elif dataset_name == 'synthetic':
        processed_data = dataset.to_list()
    else:
        # Generic processing for other datasets
        processed_data = []
        for example in dataset:
            try:
                # Convert to our format
                processed_example = {
                    'text': example.get('text', str(example)),
                    'entities': example.get('entities', []),
                    'relations': example.get('relations', []),
                    'confidence': 0.8
                }
                processed_data.append(processed_example)
            except Exception as e:
                print(f"Error processing example: {e}")
                continue
    
    # Format for training
    formatted_data = [format_for_training(ex) for ex in processed_data]
    processed_datasets.extend(formatted_data)

# Create final dataset
print(f"Total processed examples: {len(processed_datasets)}")
full_dataset = Dataset.from_list(processed_datasets)

# Split into train/test
train_test_split = full_dataset.train_test_split(test_size=1-TrainingConfig.train_test_split)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

print(f"Training examples: {len(train_dataset)}")
print(f"Evaluation examples: {len(eval_dataset)}")

# Step 7: Training setup
if TrainingConfig.use_wandb:
    wandb.init(
        project=TrainingConfig.wandb_project,
        name=TrainingConfig.wandb_run_name,
        config=vars(TrainingConfig)
    )

# Configure training arguments
training_args = {
    "output_dir": TrainingConfig.output_dir,
    "per_device_train_batch_size": TrainingConfig.batch_size,
    "gradient_accumulation_steps": TrainingConfig.gradient_accumulation_steps,
    "warmup_steps": TrainingConfig.warmup_steps,
    "max_steps": TrainingConfig.max_steps,
    "learning_rate": TrainingConfig.learning_rate,
    "weight_decay": TrainingConfig.weight_decay,
    "lr_scheduler_type": TrainingConfig.lr_scheduler_type,
    "optim": TrainingConfig.optim,
    "logging_steps": 10,
    "save_steps": 100,
    "eval_steps": 100,
    "save_total_limit": 3,
    "load_best_model_at_end": True,
    "metric_for_best_model": "loss",
    "greater_is_better": False,
    "report_to": "wandb" if TrainingConfig.use_wandb else "tensorboard",
    "run_name": TrainingConfig.wandb_run_name,
    "fp16": True,
    "gradient_checkpointing": True,
    "dataloader_num_workers": 2,
    "remove_unused_columns": False,
}

# Step 8: Start training
print("🎯 Starting training...")
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    dataset_text_field="text",
    max_seq_length=TrainingConfig.max_seq_length,
    packing=False,
    args=training_args,
)

# Train the model
trainer.train()

print("✅ Training completed")

# Step 9: Save model
print("💾 Saving model...")
trainer.save_model(TrainingConfig.final_model_dir)
tokenizer.save_pretrained(TrainingConfig.final_model_dir)

# Save training configuration
with open(f"{TrainingConfig.final_model_dir}/training_config.json", "w") as f:
    json.dump(vars(TrainingConfig), f, indent=2)

print(f"✅ Model saved to {TrainingConfig.final_model_dir}")

# Step 10: Test the model
print("🧪 Testing model...")
FastLanguageModel.for_inference(model)

def test_model(text: str):
    """Test the trained model"""
    
    system_prompt = """You are an expert relation extraction system. Extract entities and relations from the given text and output them in JSON format.

Entities should include people, organizations, locations, and other important nouns.
Relations should connect entities with meaningful predicates like 'works_for', 'located_in', 'develops', etc.

Output format:
{
  "entities": [
    {"text": "Entity Name", "type": "PERSON/ORG/LOC/PRODUCT", "confidence": 0.9}
  ],
  "relations": [
    {"subject": "Entity1", "predicate": "relation_type", "object": "Entity2", "confidence": 0.8}
  ],
  "confidence": 0.85
}"""

    user_prompt = f"Extract entities and relations from this text:\n\n{text}\n\nOutput JSON:"
    
    # Format for inference
    formatted_text = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{user_prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(formatted_text, return_tensors="pt").to("cuda")
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.1,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id
    )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract JSON portion
    try:
        json_start = generated_text.find('{')
        json_end = generated_text.rfind('}') + 1
        
        if json_start != -1 and json_end != -1:
            json_str = generated_text[json_start:json_end]
            result = json.loads(json_str)
            return result
    except:
        pass
    
    return {"entities": [], "relations": [], "confidence": 0.0}

# Test examples
test_texts = [
    "I work at Google as a software engineer in Mountain View.",
    "Sarah is friends with John and they both work at Microsoft.",
    "Apple develops the iPhone and is headquartered in Cupertino.",
    "My name is Emma and I'm a data scientist at Amazon in Seattle.",
]

print("\n🧪 Model Test Results:")
for i, text in enumerate(test_texts):
    print(f"\nTest {i+1}: {text}")
    result = test_model(text)
    print(f"Entities: {len(result.get('entities', []))}")
    print(f"Relations: {len(result.get('relations', []))}")
    print(f"Result: {json.dumps(result, indent=2)}")

# Step 11: Package for HotMem V3
print("\n📦 Packaging for HotMem V3...")

# Create package structure
package_dir = "./hotmem_v3_package"
os.makedirs(package_dir, exist_ok=True)

# Copy model files
import shutil
shutil.copytree(TrainingConfig.final_model_dir, f"{package_dir}/model", dirs_exist_ok=True)

# Create integration script
integration_script = '''"""
HotMem V3 Model Integration Script
Packaged model for seamless integration with HotMem V3 pipeline
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, Any, List, Optional
import logging

logger = logging.getLogger(__name__)

class HotMemV3Model:
    """Packaged HotMem V3 model for relation extraction"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.load_model()
    
    def load_model(self):
        """Load the trained model"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            logger.info(f"✅ HotMem V3 model loaded from {self.model_path}")
        except Exception as e:
            logger.error(f"❌ Failed to load model: {e}")
            raise
    
    def extract_relations(self, text: str) -> Dict[str, Any]:
        """Extract entities and relations from text"""
        
        system_prompt = """You are an expert relation extraction system. Extract entities and relations from the given text and output them in JSON format.

Entities should include people, organizations, locations, and other important nouns.
Relations should connect entities with meaningful predicates like 'works_for', 'located_in', 'develops', etc.

Output format:
{
  "entities": [
    {"text": "Entity Name", "type": "PERSON/ORG/LOC/PRODUCT", "confidence": 0.9}
  ],
  "relations": [
    {"subject": "Entity1", "predicate": "relation_type", "object": "Entity2", "confidence": 0.8}
  ],
  "confidence": 0.85
}"""

        user_prompt = f"Extract entities and relations from this text:\\n\\n{text}\\n\\nOutput JSON:"
        
        formatted_text = f"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_prompt}<|im_end|>\\n<|im_start|>assistant\\n"
        
        inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.device)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.1,
            do_sample=False,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract JSON portion
        try:
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = generated_text[json_start:json_end]
                result = json.loads(json_str)
                return result
        except:
            pass
        
        return {"entities": [], "relations": [], "confidence": 0.0}
    
    def __call__(self, text: str) -> Dict[str, Any]:
        """Make the model callable"""
        return self.extract_relations(text)

# Usage example
if __name__ == "__main__":
    model = HotMemV3Model("./model")
    result = model("I work at Google as a software engineer.")
    print(result)
'''

with open(f"{package_dir}/hotmem_v3_model.py", "w") as f:
    f.write(integration_script)

# Create README
readme_content = '''# HotMem V3 Package

This package contains the trained Qwen2.5-0.5B model optimized for HotMem V3 relation extraction.

## Files Structure
- `model/`: Trained model files
- `hotmem_v3_model.py`: Integration script
- `training_config.json`: Training configuration

## Usage
```python
from hotmem_v3_model import HotMemV3Model

# Load model
model = HotMemV3Model("./model")

# Extract relations
result = model("I work at Google as a software engineer.")
print(result)
```

## Model Performance
- Model: Qwen2.5-0.5B-Instruct
- Training: Optimized for conversational text
- Output: JSON with entities and relations
- Format: Compatible with HotMem V3 pipeline

## Integration
1. Copy this package to `/optimized_models/hotmem_v3_package/`
2. Update model path in HotMem V3 configuration
3. Restart the system
'''

with open(f"{package_dir}/README.md", "w") as f:
    f.write(readme_content)

print(f"✅ Package created at {package_dir}")

# Step 12: Final summary
print("\n" + "="*60)
print("🎉 HOTMEM V3 TRAINING COMPLETED!")
print("="*60)
print(f"📊 Training Summary:")
print(f"  - Model: {TrainingConfig.model_name}")
print(f"  - Training steps: {TrainingConfig.max_steps}")
print(f"  - Batch size: {TrainingConfig.batch_size}")
print(f"  - Learning rate: {TrainingConfig.learning_rate}")
print(f"  - Dataset size: {len(train_dataset)} examples")
print(f"  - Model saved to: {TrainingConfig.final_model_dir}")
print(f"  - Package created: {package_dir}")
print(f"\n🚀 Next Steps:")
print(f"  1. Download the package folder")
print(f"  2. Extract to /optimized_models/hotmem_v3_package/")
print(f"  3. Update HotMem V3 configuration")
print(f"  4. Test with localcat pipeline")
print(f"\n📈 Monitor training progress:")
if TrainingConfig.use_wandb:
    print(f"  - WandB: https://wandb.ai/{TrainingConfig.wandb_project}")
print("="*60)

# Finish WandB run
if TrainingConfig.use_wandb:
    wandb.finish()
'''

print("✅ HotMem V3 Training Colab script created successfully!")
print("📂 File saved as: hotmem_v3_training_colab.ipynb")
print("🚀 Ready for upload to Google Colab!")