# Copy to server/.env and customize for your setup
# 
# OPTIMAL CONFIGURATION - Tested and validated
# This configuration provides:
# - Working temporal decay (recent facts dominate)
# - Real-time correction system (67%+ success rate)  
# - Enhanced retrieval with summaries + semantic search
# - Session isolation to prevent context pollution
#########################

#########################
# IDs & Models
#########################
USER_ID=peppi
AGENT_ID=locat

# OpenAI-compatible API (Ollama optimized)
OPENAI_BASE_URL=http://127.0.0.1:11434/v1
OPENAI_API_KEY=not-needed
OPENAI_MODEL=huihui_ai/qwen2.5-abliterate:3b # Recommended: fast + capable
EMBEDDING_MODEL=nomic-embed-text:latest

# Ollama thinking mode controls (experimental/model-dependent)
# Note: No official env var exists yet - these are for future compatibility
# OLLAMA_DISABLE_THINKING=true  # Future: disable thinking globally
# OLLAMA_ENABLE_THINKING=false  # Future: model-specific thinking control
# Current workaround: use /nothink command during inference or higher temperature

#########################
# Smart Turn / VAD
#########################
LOCAL_SMART_TURN_MODEL_PATH=/abs/path/to/smart-turn
ENABLE_MEMORY=true

#########################
# HotMem Storage & Logs
#########################
# Paths are resolved relative to server/; these point to repo-root data/
HOTMEM_SQLITE=../data/memory.db
HOTMEM_LMDB_DIR=../data/graph.lmdb

HOTMEM_LOG_FILE=server/.logs/hotmem.log
HOTMEM_CONSOLE_DEBUG=true
HOTMEM_LOG_LEVEL=DEBUG
HOTMEM_TRACE_FRAMES=false

# Injection formatting
HOTMEM_INJECT_ROLE=system
HOTMEM_INJECT_HEADER=Use the following factual context if helpful.
# Bullet cap per turn (1–5)
HOTMEM_BULLETS_MAX=5

#########################
# Session Summary & LEANN
#########################
SESSION_SUMMARY_ENABLED=true
REBUILD_LEANN_ON_SESSION_END=true

# Semantic retrieval (optional). Requires `pip install leann`.
HOTMEM_USE_LEANN=false
LEANN_INDEX_PATH=../data/memory_vectors.leann
LEANN_BACKEND=hnsw
HOTMEM_LEANN_COMPLEXITY=16

#########################
# Extraction Quality (Phase 2)
#########################
# Enable clause decomposition for complex sentences
HOTMEM_DECOMPOSE_CLAUSES=false
# Add conservative complexity-aware confidence scoring
HOTMEM_EXTRA_CONFIDENCE=false
# Minimum confidence required to store a fact
HOTMEM_CONFIDENCE_THRESHOLD=0.3
HOTMEM_BYPASS_CONFIDENCE_FOR_BASIC=true
HOTMEM_CONFIDENCE_FLOOR_BASIC=0.6

#########################
# SRL (Semantic Roles) — Experimental
#########################
# Prefer SRL-first extraction and fuse with UD patterns
HOTMEM_USE_SRL=false
# Optional multilingual embedding model for relation normalization
# Defaults to paraphrase-multilingual-MiniLM-L12-v2 when enabled
HOTMEM_REL_EMBED_MODEL=

#########################
# ONNX NER / SRL (Advanced, fully local)
#########################
# Enable ONNX NER to enrich entity mapping
HOTMEM_USE_ONNX_NER=false
HOTMEM_ONNX_NER_MODEL=             # /abs/path/to/ner.onnx
HOTMEM_ONNX_NER_LABELS=            # /abs/path/to/ner_labels.txt   (BIO labels)
HOTMEM_ONNX_NER_TOKENIZER=bert-base-cased

# Enable ONNX SRL to extract roles (ARG0/ARG1/ARGM-*)
HOTMEM_USE_ONNX_SRL=false
HOTMEM_ONNX_SRL_MODEL=             # /abs/path/to/srl.onnx
HOTMEM_ONNX_SRL_LABELS=            # /abs/path/to/srl_labels.txt   (BIO SRL tags)
HOTMEM_ONNX_SRL_TOKENIZER=bert-base-cased

#########################
# Coreference (Neural) — Optional
#########################
HOTMEM_USE_COREF=false
HOTMEM_COREF_DEVICE=cpu

#########################
# ReLiK (Relation Extraction) — Optional
#########################
HOTMEM_USE_RELIK=false
HOTMEM_RELIK_MODEL_ID=relik-ie/relik-relation-extraction-small
HOTMEM_RELIK_DEVICE=cpu
HOTMEM_RELIK_TOP_K=8
HOTMEM_RELIK_WINDOW_SIZE=48
HOTMEM_RELIK_WINDOW_STRIDE=32
HOTMEM_RELIK_MAX_CHARS=480
HOTMEM_RELIK_ENABLE_INTENTS=fact_statement,multiple_facts,correction

#########################
# Periodic Summarizer (LLM)
#########################
SUMMARIZER_ENABLED=true
SUMMARIZER_BASE_URL=http://127.0.0.1:1234/v1   # LM Studio default
SUMMARIZER_API_KEY=
SUMMARIZER_MODEL=qwen3:4b
SUMMARIZER_INTERVAL_SECS=30
SUMMARIZER_MAX_TOKENS=160

# Windowing: summarize new messages since last run (delta), or last N turn pairs
SUMMARIZER_WINDOW_MODE=delta   # delta | turn_pairs | tail
SUMMARIZER_TURN_PAIRS=2        # when WINDOW_MODE=turn_pairs
SUMMARIZER_MAX_MESSAGES=16
SUMMARIZER_INCLUDE_USER=true
SUMMARIZER_INCLUDE_ASSISTANT=true

# Reasoning controls for summarizer (best-effort)
# Set SUMMARIZER_THINK to true|false|low|medium|high (Ollama-compatible)
SUMMARIZER_THINK=
SUMMARIZER_REASONING_TOKENS=
SUMMARIZER_NUM_CTX=
