{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ HotMem V3 Cloud Training\n",
        "\n",
        "This notebook trains HotMem V3 using cloud GPUs (Google Colab)\n",
        "\n",
        "## Setup Instructions:\n",
        "1. **Set Runtime**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
        "2. **Run All**: Click Runtime ‚Üí Run all\n",
        "3. **Download**: Download trained model when complete\n",
        "\n",
        "## Expected Time:\n",
        "- Data preparation: 15-20 minutes\n",
        "- Training: 1-2 hours (demo) or 6-8 hours (full)\n",
        "- Download: 5 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "print(\"üîß Installing dependencies...\")\n",
        "!pip install -q unsloth datasets torch accelerate\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q bitsandbytes trl\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare datasets\n",
        "print(\"üìö Loading datasets...\")\n",
        "\n",
        "# Load smaller subsets for demo training\n",
        "rebel = load_dataset(\"Babelscape/rebel-dataset\", split=\"train[:5000]\")  # 5K for demo\n",
        "dialogre = load_dataset(\"dialogre\", split=\"train[:3000]\")  # 3K for demo\n",
        "convquestions = load_dataset(\"convquestions\", split=\"train[:2000]\")  # 2K for demo\n",
        "\n",
        "print(f\"‚úÖ Loaded datasets:\")\n",
        "print(f\"   - REBEL: {len(rebel)} examples\")\n",
        "print(f\"   - DialogRE: {len(dialogre)} examples\")\n",
        "print(f\"   - ConvQuestions: {len(convquestions)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert datasets to training format\n",
        "print(\"üîÑ Converting datasets to training format...\")\n",
        "\n",
        "def convert_rebel_to_training(rebel_data):\n",
        "    training_examples = []\n",
        "    \n",
        "    for item in tqdm(rebel_data):\n",
        "        text = item.get('text', '')\n",
        "        triples = item.get('triples', [])\n",
        "        \n",
        "        if text and triples:\n",
        "            entities = set()\n",
        "            relations = []\n",
        "            \n",
        "            for triple in triples:\n",
        "                if isinstance(triple, dict):\n",
        "                    entities.add(triple.get('head', ''))\n",
        "                    entities.add(triple.get('tail', ''))\n",
        "                    relations.append({\n",
        "                        'subject': triple.get('head', ''),\n",
        "                        'predicate': triple.get('type', ''),\n",
        "                        'object': triple.get('tail', '')\n",
        "                    })\n",
        "            \n",
        "            training_examples.append({\n",
        "                'text': text,\n",
        "                'entities': list(entities),\n",
        "                'relations': relations,\n",
        "                'domain': 'general'\n",
        "            })\n",
        "    \n",
        "    return training_examples\n",
        "\n",
        "# Convert datasets\n",
        "rebel_training = convert_rebel_to_training(rebel)\n",
        "print(f\"‚úÖ Converted REBEL: {len(rebel_training)} examples\")\n",
        "\n",
        "# Combine all training data\n",
        "all_training_data = rebel_training  # Add more datasets as needed\n",
        "print(f\"üìä Total training examples: {len(all_training_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Qwen2.5-0.5B model with Unsloth\n",
        "print(\"üîß Setting up Qwen2.5-0.5B with Unsloth...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        "    dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model ready for training\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format data for training\n",
        "print(\"üìù Formatting data for Qwen2.5 training...\")\n",
        "\n",
        "def format_for_qwen(examples):\n",
        "    formatted_examples = []\n",
        "    \n",
        "    for example in examples[:1000]:  # Use subset for demo\n",
        "        prompt = f\"\"\"Extract entities and relations from the following text. Output in JSON format.\n",
        "\n",
        "Text: {example['text']}\n",
        "\n",
        "Output JSON:\n",
        "\"\"\"\n",
        "        \n",
        "        output = {\n",
        "            \"entities\": example['entities'],\n",
        "            \"relations\": example['relations'],\n",
        "            \"confidence\": 0.9\n",
        "        }\n",
        "        \n",
        "        formatted_examples.append({\n",
        "            'instruction': 'Extract entities and relations as JSON',\n",
        "            'input': example['text'],\n",
        '            'output': json.dumps(output, indent=2),\n",
        "        })\n",
        "    \n",
        "    return formatted_examples\n",
        "\n",
        "formatted_data = format_for_qwen(all_training_data)\n",
        "print(f\"‚úÖ Formatted {len(formatted_data)} examples for training\")\n",
        "\n",
        "# Save formatted data\n",
        "with open('hotmem_training_data.json', 'w') as f:\n",
        "    json.dump(formatted_data, f, indent=2)\n",
        "print(\"üíæ Saved training data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup training\n",
        "print(\"üèÉ Setting up training...\")\n",
        "\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    num_train_epochs=1,  # Demo training - increase for better results\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"hotmem_v3_outputs\",\n",
        "    report_to=\"none\",  # Disable wandb for simplicity\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"‚è±Ô∏è Expected training time: ~30 minutes for demo\")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "print(\"‚úÖ Training completed!\")\n",
        "\n",
        "# Display training stats\n",
        "print(f\"üìä Training stats:\")\n",
        "print(f\"   - Total training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"   - Training loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
        "print(f\"   - Samples per second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "print(\"üß™ Testing trained model...\")\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Enable inference mode\n",
        "\n",
        "test_texts = [\n",
        "    \"John works at Google in Mountain View\",\n",
        "    \"Sarah has a dog named Max who loves playing fetch\",\n",
        "    \"Microsoft was founded by Bill Gates and Paul Allen\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    prompt = f\"Extract entities and relations from: {text}\\nOutput JSON:\\n\"\n",
        "    \n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        use_cache=True,\n",
        "        temperature=0.1,\n",
        "        do_sample=False\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"\\nüìù Input: {text}\")\n",
        "    print(f\"ü§ñ Response: {response.split('Output JSON:')[-1].strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model for download\n",
        "print(\"üíæ Saving model for download...\")\n",
        "\n",
        "# Save to 16bit for inference\n",
        "model.save_pretrained_merged(\"hotmem_v3_qwen\", tokenizer, save_method=\"merged_16bit\")\n",
        "print(\"‚úÖ Saved 16-bit model to hotmem_v3_qwen\")\n",
        "\n",
        "# Save to GGUF for llama.cpp\n",
        "model.save_pretrained_gguf(\"hotmem_v3\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "print(\"‚úÖ Saved GGUF model to hotmem_v3\")\n",
        "\n",
        "# Create zip file for download\n",
        "!zip -r hotmem_v3_model.zip hotmem_v3_qwen/\n",
        "print(\"‚úÖ Created zip file: hotmem_v3_model.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the model\n",
        "print(\"üì• Downloading model...\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('hotmem_v3_model.zip')\n",
        "files.download('hotmem_v3-*.gguf')\n",
        "\n",
        "print(\"‚úÖ Model files ready for download!\")\n",
        "print(\"\\nüéâ Training complete!\")\n",
        "print(\"\\nNext steps:\")\n",
        "1. Download the model files\n",
        "2. Copy to your Mac at ~/localcat/models/\")\n",
        "3. Update HotMem v3 to use the new model\n",
        "4. Test with your voice AI pipeline\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HotMem V3 Cloud Training",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}